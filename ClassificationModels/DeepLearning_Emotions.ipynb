{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6F7sYiTedSz"
   },
   "source": [
    "# 1. Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22v8oPQVeLJV"
   },
   "source": [
    "**Import some libraries and the Corpus (Mental Health dataset) stored in the drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMDT_JUiZ98-",
    "outputId": "924d3a0e-6992-423e-be44-462955419e50"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style(style = 'whitegrid')\n",
    "%matplotlib inline\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ycOQtTJfkDq"
   },
   "source": [
    "**Analysis of the Mental Health Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "6SdIn55UeVGj",
    "outputId": "987b4785-1676-452b-fc89-02467253bf8d"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/content/drive/MyDrive/DatasetMH_Emotions.csv\", sep = \";\", header = None)\n",
    "\n",
    "# I remove the first line with the heading\n",
    "dataset.drop(dataset.head(1).index,inplace = True)\n",
    "dataset.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60MGSE8qxeU8"
   },
   "source": [
    "**Columns are named to organise Instragram messages with their corresponding emotion (class o category)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDxY3YYmeweV"
   },
   "outputs": [],
   "source": [
    "dataset.columns = [\"Id\", \"Emoticos\", \"Polaridad\",\"Emocion\", \"nada\", \"nada\", \"nada\"]\n",
    "\n",
    "#Label Polaridad is Polarity\n",
    "#Label Emoticonos is Emoticons\n",
    "#Label nada is null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "KLZ3vR-iAnQG",
    "outputId": "6b495616-35b4-46a4-c5e7-e6a197fb1cac"
   },
   "outputs": [],
   "source": [
    "# Check whether the first column of the dataset has been removed\n",
    "dataset.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mN1UFiWxi6A"
   },
   "source": [
    "**Graph to show the distribution of messages in the dataset according to emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 873
    },
    "id": "cK4D-GFse3Zl",
    "outputId": "6ac11fca-636b-49a2-9692-67dfcaf13f69"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "sns.countplot(x = 'Emocion', data = dataset, palette = 'rocket',\n",
    "\n",
    "              order=['Amor/AdmiraciÃ³n', 'Gratitud', 'ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n', 'Tristeza/Pena','Enfado/Desprecio/Burla','Indeterminado']);\n",
    "plt.xlabel('Polaridad')\n",
    "plt.ylabel('Number of messages')\n",
    "plt.title('Polarity distribution of messages')\n",
    "plt.show()\n",
    "\n",
    "#Label Positiva is Positive\n",
    "#Label Negativa is Negative\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "#Label Amor/AdmiraciÃ³n is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUXEDGeox1_q"
   },
   "source": [
    "**6 datasets are created, one for each class to print the number of messages of each class, in this case of each emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2VMqXAte4bb",
    "outputId": "11aed6c4-609c-4329-eacd-6644183d1bb1"
   },
   "outputs": [],
   "source": [
    "#Label Amor/AdmiraciÃ³n is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "dataset_Love = dataset[dataset['Emocion'] == 'Amor/AdmiraciÃ³n']\n",
    "dataset_Gratitude = dataset[dataset['Emocion'] == 'Gratitud']\n",
    "dataset_Empathy = dataset[dataset['Emocion'] == 'ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n']\n",
    "dataset_Sadness = dataset[dataset['Emocion'] == 'Tristeza/Pena']\n",
    "dataset_Anger = dataset[dataset['Emocion'] == 'Enfado/Desprecio/Burla']\n",
    "dataset_Neutral = dataset[dataset['Emocion'] == 'Indeterminado']\n",
    "\n",
    "print(\"Number of messages with the following emotion:\\n\",\n",
    "      \"\\nLove   \", len(dataset_Love),\n",
    "      \"\\nGratitud   \", len(dataset_Gratitude),\n",
    "      \"\\nComprenhesion   \", len(dataset_Empathy),\n",
    "      \"\\nSadness   \", len(dataset_Sadness),\n",
    "      \"\\nAnger   \", len(dataset_Anger),\n",
    "      \"\\nNeutral   \", len(dataset_Neutral))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvTCn0DTx1Mr"
   },
   "source": [
    "**Dataset of Mental Healt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "YfcB_kage4jT",
    "outputId": "301d269b-9ec4-43ba-b65e-88b2de9c358a"
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset_Love, dataset_Gratitude, dataset_Empathy,\n",
    "                     dataset_Sadness, dataset_Anger, dataset_Neutral], axis = 0)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soQ1bSLnyazT"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArOmSkXwf3a8",
    "outputId": "69298778-743f-4868-dfab-6eb81ed782ce"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph8kFdvCgGEr"
   },
   "source": [
    "# 3. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3UBKGLdyfUa"
   },
   "source": [
    "**Pre-processing and tokenisation function of dataset messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sNguvKNf3ia"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk import TweetTokenizer\n",
    "import spacy\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# We create the function 'processed' which will delete stopwords and some characters peculiar to social networks\n",
    "def processing(text):\n",
    "  #Spanish stopwords \n",
    "  stopWords_without_prepositions = {'al', 'algo', 'algunas', 'algunos', 'antes', 'como', 'cual', 'cuando', 'del', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
    "                               'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estarÃ¡', 'estarÃ¡n', 'estarÃ¡s', 'estarÃ©', 'estarÃ©is', 'estarÃ­a', 'estarÃ­ais', 'estarÃ­amos', 'estarÃ­an', 'estarÃ­as', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve',\n",
    "                               'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviÃ©ramos', 'estuviÃ©semos', 'estuvo', 'estÃ¡', 'estÃ¡bamos', 'estÃ¡is', 'estÃ¡n', 'estÃ¡s', 'estÃ©', 'estÃ©is',\n",
    "                               'estÃ©n', 'estÃ©s', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuÃ©ramos', 'fuÃ©semos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrÃ¡', 'habrÃ¡n', 'habrÃ¡s',\n",
    "                               'habrÃ©', 'habrÃ©is', 'habrÃ­a', 'habrÃ­ais', 'habrÃ­amos', 'habrÃ­an', 'habrÃ­as', 'habÃ©is', 'habÃ­a', 'habÃ­ais', 'habÃ­amos', 'habÃ­an', 'habÃ­as', 'han', 'has', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayÃ¡is', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras',\n",
    "                               'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiÃ©ramos', 'hubiÃ©semos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'mÃ¡s', 'mÃ­', 'mÃ­a', 'mÃ­as', 'mÃ­o', 'mÃ­os', 'nada', 'nos',\n",
    "                               'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'pero', 'poco', 'porque', 'que', 'quien', 'quienes', 'quÃ©', 'se', 'sea', 'seamos', 'sean', 'seas', 'sentid', 'sentida', 'sentidas', 'sentido', 'sentidos', 'seremos',\n",
    "                               'serÃ¡', 'serÃ¡n', 'serÃ¡s', 'serÃ©', 'serÃ©is', 'serÃ­a', 'serÃ­ais', 'serÃ­amos', 'serÃ­an', 'serÃ­as', 'seÃ¡is', 'siente', 'sintiendo', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sÃ­', 'tambiÃ©n', 'tanto', 'te', 'tendremos', 'tendrÃ¡', 'tendrÃ¡n', 'tendrÃ¡s',\n",
    "                               'tendrÃ©', 'tendrÃ©is', 'tendrÃ­a', 'tendrÃ­ais', 'tendrÃ­amos', 'tendrÃ­an', 'tendrÃ­as', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengÃ¡is', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenÃ©is', 'tenÃ­a', 'tenÃ­ais', 'tenÃ­amos', 'tenÃ­an', 'tenÃ­as',\n",
    "                               'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviÃ©ramos', 'tuviÃ©semos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos',\n",
    "                               'tÃº', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', 'Ã©l', 'Ã©ramos'}\n",
    "  #Translation into english  of stopwords\n",
    "  #stopWords_without_prepositions = {\"'to', 'something', 'some', 'some', 'before', 'like', 'which', 'when', 'of the', 'where', 'during', 'and', 'the', 'she', 'they', 'they', 'he', 'were', 'were', 'were', 'were', 'is', 'that', 'those', 'that', 'that', 'those', 'this', 'was', 'were', 'were', 'were', 'you were', 'be', 'we will', 'will be', 'will be', 'will be', 'will be', 'I will be', 'will be', 'would be', 'would be', 'would be', 'would be', 'would be', 'these', 'this', 'let us', 'this', 'these', 'I am', 'I was', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'was', 'were', 'were', 'were', 'were', 'were', 'was', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were',}\n",
    "\n",
    "\n",
    "  DIACRITICAL_VOWELS = [('Ã¡','a'), ('Ã©','e'), ('Ã­','i'), ('Ã³','o'), ('Ãº','u'), ('Ã¼','u')]\n",
    "\n",
    "\n",
    "    \n",
    "  SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
    "           ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'tambiÃ©n'),\n",
    "           ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]\n",
    "  #Translation into english o SLANG\n",
    "  #SLANG = [(â€˜dâ€™,â€˜ofâ€™), (â€˜[qk]â€™,â€˜whatâ€™), (â€˜xoâ€™,â€˜butâ€™), (â€˜xaâ€™,â€˜forâ€™), (â€˜[xp]qâ€™,â€˜whyâ€™),(â€˜es[qk]â€™,â€˜is thatâ€™),\n",
    "           #(â€˜fvrâ€™, â€˜pleaseâ€™),(â€˜(xfa|xf|pf|plis|pls|please)â€™, â€˜pleaseâ€™), (â€˜dndâ€™, â€˜whereâ€™), (â€˜tbâ€™, â€˜alsoâ€™),\n",
    "           #(â€˜(tq|tk)â€™, â€˜I love youâ€™), (â€˜(tqm|tkm)â€™, â€˜I love you very muchâ€™), (â€˜xâ€™, â€˜forâ€™), (â€˜+â€™, â€˜moreâ€™)]\n",
    "    \n",
    "  # Delete mentions  @, # , links...\n",
    "  text = str(text)\n",
    "  text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
    "  text = re.sub(r'RT[|\\s]', ' ', text)\n",
    "  text = re.sub(r'#', ' ', text)\n",
    "  text = re.sub(r'https?:\\/\\/\\S+', ' ', text)\n",
    "\n",
    "  stemming = True\n",
    "\n",
    "  lemmatization = False\n",
    "\n",
    "  #Stemming in spanish\n",
    "  _stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "  #lemmatisation in Spanish\n",
    "  #nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "  # Message tokeniser (we use this one from Twitter)\n",
    "  _tokenizer = TweetTokenizer().tokenize\n",
    "\n",
    "  _stemming = stemming\n",
    "\n",
    "\n",
    "  # Convert to lower case\n",
    "  text = text.lower()\n",
    "\n",
    "\n",
    "  # Delete numbers and carriage returns\n",
    "  text = re.sub(r'(\\d+|\\n)', '', text)\n",
    "\n",
    "  # Deleting vowels with diacritical marks\n",
    "  for s,t in DIACRITICAL_VOWELS:\n",
    "    text = re.sub(r'{0}'.format(s), t, text)\n",
    "\n",
    "  # Delete repeated characters\n",
    "  text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "  # Normalise laughter and replace with predefined variables\n",
    "  text = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', ' ', text)\n",
    "  text = re.sub(r'\\b(juas+|lol)\\b', ' ', text)\n",
    "\n",
    "  # translate slang\n",
    "  for s,t in SLANG:\n",
    "    text = re.sub(r'\\b{0}\\b'.format(s), t, text)\n",
    "\n",
    "\n",
    "  pattern = r'''(?x)                  # Set flag to allow verbose regexps\n",
    "              (?:[A-Z]\\.)+            # Abbreviations, e.g. U.S.A \n",
    "              | \\w+(?:-\\w+)*          # Words with optional internal hyphens\n",
    "              | \\$?\\d+(?:\\.\\d+)?%?    # Currency and precentages, e.g. $12.40 82% \n",
    "              | \\.\\.\\.                # Ellipsis \n",
    "              | [][.,;\"'?():-_`]      # These are separate tokens; includes \n",
    "              | [ğŸ˜€\\ğŸ˜\\ğŸ˜‚\\ğŸ¤£\\ğŸ˜ƒ\\ğŸ˜„\\ğŸ˜…\\ğŸ˜†\\ğŸ˜‰\\ğŸ˜Š\\ğŸ˜‹\\ğŸ˜\\ğŸ˜\\ğŸ˜˜\\ğŸ˜—\\ğŸ˜™\\ğŸ˜š\\â˜º\\ğŸ™‚\\ğŸ¤—\\ğŸ¤©\\ğŸ˜Œ\\ğŸ˜›\\ğŸ˜œ\\ğŸ˜\\ğŸ¤¤\\ğŸ¤‘\\ğŸ˜‡\\ğŸ¤­\\ğŸ˜º\\ğŸ˜¸\\ğŸ˜¹\\ğŸ˜»\\ğŸ˜½\\ğŸ’ª\\âœŒ\\ğŸ–\\âœ‹\\ğŸ‘Œ\\ğŸ‘\\ğŸ‘‹\\ğŸ‘\\ğŸ™Œ\\ğŸ™\\ğŸ’‹\\ğŸ’˜\\â¤\\ğŸ’“\\ğŸ’”\\ğŸ’•\\ğŸ’–\\ğŸ’—\\ğŸ’™\\ğŸ’š\\ğŸ’›\\ğŸ§¡\\ğŸ’œ\\ğŸ–¤\\ğŸ’\\ğŸ’\\ğŸ’Ÿ\\â£\\ğŸ’Œ\\ğŸº\\ğŸ»\\ğŸ‰\\ğŸŠ\\ğŸ™‹\\ğŸ•º\\ğŸ’ƒ] # \\:d\\:)\\:-)\\:-d\\;d\\;-)\\=d\\;)\\:]\\:-]\\=)\\=]\\(:\\xd\\:p\\:-p\\8)\\xp\\<3\n",
    "              | [:d]\n",
    "              | [:)]\n",
    "              #| [:-)]\n",
    "              | [:-d]\n",
    "              | [;d]\n",
    "              #| [;-)]\n",
    "              | [=d]\n",
    "              | [;)]\n",
    "              | [:]]\n",
    "              | [:-]]\n",
    "              | [=)]\n",
    "              | [=]]\n",
    "              | [(:]\n",
    "              | [xd]\n",
    "              | [:p]\n",
    "              | [:-p]\n",
    "              | [8)]\n",
    "              | [xp]\n",
    "              | [<3]\n",
    "              | [ğŸ¤”\\ğŸ¤¨\\ğŸ˜\\ğŸ˜‘\\ğŸ˜¶\\ğŸ™„\\ğŸ˜\\ğŸ˜®\\ğŸ¤\\ğŸ˜¯\\ğŸ˜’\\ğŸ˜•\\ğŸ™ƒ\\ğŸ˜²\\ğŸ˜¼\\ğŸ¤·] # \\:-|\\:|]\n",
    "              | [:-|]\n",
    "              | [:|]\n",
    "              | [ğŸ˜£\\ğŸ˜¥\\ğŸ˜ª\\ğŸ˜«\\ğŸ˜“\\ğŸ˜”\\â˜¹\\ğŸ™\\ğŸ˜–\\ğŸ˜\\ğŸ˜Ÿ\\ğŸ˜¤\\ğŸ˜¢\\ğŸ˜­\\ğŸ˜¦\\ğŸ˜§\\ğŸ˜¨\\ğŸ˜©\\ğŸ¤¯\\ğŸ˜¬\\ğŸ˜°\\ğŸ˜±\\ğŸ˜³\\ğŸ˜µ\\ğŸ˜¡\\ğŸ˜ \\ğŸ¤¬\\ğŸ˜·\\ğŸ¤’\\ğŸ¤•\\ğŸ¤¢\\ğŸ¤®\\ğŸ¤§\\ğŸ’©\\ğŸ™€\\ğŸ˜¿\\ğŸ˜¾\\ğŸ–•\\ğŸ‘\\â›”\\ğŸš«\\ğŸ¤¦] # \\:-(\\:(\\:-<\\:<\\:-[\\:[\\>:-[\\>:[\\:-{\\:{\\:-@\\:@\\>:-(\\>:(\\:-(\\:(\\d:\\:\\\\:/\\:-/\\:-\\\\dx\\d8\n",
    "              #| [:-(]\n",
    "              #| [:(]\n",
    "              | [:-<]\n",
    "              | [:<]\n",
    "              | [:-[]\n",
    "              | [:[]\n",
    "              | [>:-[]\n",
    "              | [>:[]\n",
    "              | [:-{]\n",
    "              | [:{]\n",
    "              | [:-@]\n",
    "              | [:@]\n",
    "              #| [>:-(]\n",
    "              | [>:(]\n",
    "              | [:'-(]\n",
    "              | [:'(]\n",
    "              | [d:]\n",
    "              | [:\\]\n",
    "              | [:/]\n",
    "              #| [:-/]\n",
    "              | [:-\\]\n",
    "              | [dx]\n",
    "              | [d8]\n",
    "              '''\n",
    "\n",
    "  if _stemming:\n",
    "    text = ' '.join(_stemmer.stem(w) for w in _tokenizer(text))\n",
    "\n",
    "  if lemmatization:\n",
    "    text_aux=nlp(text)\n",
    "    for word in text_aux:\n",
    "      text+=str(word.lemma_)+\" \"\n",
    "\n",
    "\n",
    "  words = nltk.regexp_tokenize(text, pattern)\n",
    "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))      # Remove punctuation marks\n",
    "\n",
    "  stripped = [re_punc.sub('', w) for w in words]                    # Remove stopwords\n",
    "\n",
    "\n",
    "  text = [w for w in stripped if  w.lower() not in stopWords_without_prepositions]\n",
    "\n",
    "\n",
    "  return (\" \".join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7HCTeRxgN1M"
   },
   "source": [
    "**The processed function is applied to each Instagram comment in the corpus. We process the data and apply stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JuV8R9Tf3sC",
    "outputId": "62d78c09-ae1c-4d9d-bdd5-a3283b9bc700"
   },
   "outputs": [],
   "source": [
    "dataset['Id'] = dataset['Id'].apply(processing)\n",
    "dataset['Id'] = dataset['Id'].str.lower()\n",
    "dataset[\"Id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07ZxNtn3yzSy"
   },
   "source": [
    "**Create the word cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "Gks181obEfAy",
    "outputId": "dcbcddb3-32f4-4165-b066-c62737ee58e4"
   },
   "outputs": [],
   "source": [
    "#wordCloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Label Emocion is Emotion\n",
    "\n",
    "#Label Amor/AdmiraciÃ³n is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "dataset_A = dataset[dataset['Emocion'] == 'Amor/AdmiraciÃ³n']\n",
    "dataset_D = dataset[dataset['Emocion'] == 'Gratitud']\n",
    "dataset_F = dataset[dataset['Emocion'] == 'ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n']\n",
    "dataset_J = dataset[dataset['Emocion'] == 'Tristeza/Pena']\n",
    "dataset_SAD = dataset[dataset['Emocion'] == 'Enfado/Desprecio/Burla']\n",
    "dataset_OTHERS = dataset[dataset['Emocion'] == 'Indeterminado']\n",
    "\n",
    "tweets= dataset_J[\"Id\"].head(70).values\n",
    "tweets=str(tweets)\n",
    "print(len(tweets))\n",
    "stop_words_sp = set(stopwords.words('spanish'))\n",
    " \n",
    "wordcloudimage = WordCloud(\n",
    "                          max_words=50,\n",
    "                          max_font_size=500,\n",
    "                          font_step=2,\n",
    "                          stopwords=stop_words_sp,\n",
    "                          background_color='white',\n",
    "                          width=1000,\n",
    "                          height=720\n",
    "                          ).generate(tweets)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloudimage)\n",
    "wordcloudimage\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfMSUaY3CeY0"
   },
   "source": [
    "**Vocabulary creation with 6 classes (5 emotions + Neutral class)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6WMjRlDn7Dm",
    "outputId": "683e46fd-b90f-4024-d02a-5a540375344d"
   },
   "outputs": [],
   "source": [
    "#Label Emocion is Emotion\n",
    "\n",
    "#Label Amor/AdmiraciÃ³n is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "vocab={}\n",
    "etiquetas=['Amor/AdmiraciÃ³n','Gratitud', 'ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n', 'Tristeza/Pena', 'Enfado/Desprecio/Burla','Indeterminado']\n",
    "numero_clases=len(etiquetas)\n",
    "_tokenizer = TweetTokenizer().tokenize\n",
    "\n",
    "for i in dataset.index:\n",
    "  emotion=dataset[\"Emocion\"][i]\n",
    "  if emotion ==\"Amor/AdmiraciÃ³n\":\n",
    "    position=0\n",
    "  elif emotion == \"Gratitud\":\n",
    "    posicion=1\n",
    "  elif emotion == \"ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n\":\n",
    "    position=2\n",
    "  elif emotion == \"Tristeza/Pena\":\n",
    "    position=3\n",
    "  elif emotion == \"Enfado/Desprecio/Burla\":\n",
    "    position=4\n",
    "  else:\n",
    "    position=5\n",
    "  for word in _tokenizer(dataset[\"Id\"][i]):\n",
    "    if word not in vocab.keys():\n",
    "      vocab[word]=[0] * 6 #con esto lo vuelvo una lista de 7 numeros enteros\n",
    "      vocab[word][position] = 1\n",
    "    else:\n",
    "      vocab[word][position] += 1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqAMAb5HCluZ"
   },
   "source": [
    "**Entropy and information gain formulas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swFqxKxjeLI-"
   },
   "outputs": [],
   "source": [
    "def entropy(probs, adjust=1e-15):\n",
    "  total=0\n",
    "  for prob in probs:\n",
    "    if(prob>0):\n",
    "      total+= (prob + adjust) * np.math.log(prob+adjust,2)\n",
    "\n",
    "  return total\n",
    "\n",
    "\n",
    "def IG(corpus_probs, word_weigths, word_probs):\n",
    "\n",
    "  corpus_entropy= entropy(corpus_probs)\n",
    "  word_entropy=0\n",
    "\n",
    "  for i in range(len(word_weigths)):\n",
    "    #print(i)\n",
    "    word_entropy+= (word_weigths[i]* entropy(word_probs[i]))\n",
    "  return corpus_entropy - word_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfAAt5zoC97B"
   },
   "source": [
    "**Calculation of the IG of each word in the corpus for the classes (6 emotions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-n7JbAS46H2Z",
    "outputId": "c91eb53d-cd6a-4d36-99bc-c887b6537a14"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "tweets_Love = len(dataset_Love)\n",
    "tweets_Gratitude = len(dataset_Gratitude)\n",
    "tweets_Empathy = len(dataset_Empathy)\n",
    "tweets_Sandness = len(dataset_Sandess)\n",
    "tweets_Anger = len(dataset_Anger)\n",
    "tweets_Neutral = len(dataset_Neutral)\n",
    "tweets_total =tweets_Love + tweets_Gratitude + tweets_Empathy + tweets_Sadness + tweets_Anger + tweets_Neutral\n",
    "\n",
    "class_counts = [ tweets_Love, tweets_Gratitude, tweets_Empathy, tweets_Sadness, tweets_Anger,\n",
    "                tweets_Neutral]\n",
    "class_probs = np.array(class_counts) / tweets_total\n",
    "\n",
    "vocab_entropy = {}\n",
    "for word in vocab.keys():\n",
    "\n",
    "    # Frequency of the word in the corpus\n",
    "    wc1 = sum(vocab[word])\n",
    "\n",
    "    # Frequency of the word not being in the corpus\n",
    "    wc0 = tweets_totales - wc1\n",
    "\n",
    "    # Probabilities of the word to be in each of the classes\n",
    "    probs_1 = [vocab[word][i] / wc1 for i in range(len(vocab[word]))]\n",
    "\n",
    "    # Probabilities of the word not being in each of the classes\n",
    "    probs_0 = [(class_counts[i] - vocab[word][i]) / (tweets_total - wc1) for i in range(len(vocab[word]))]\n",
    "\n",
    "    # Probabilities of the word being in a message\n",
    "    p_word = wc1 / tweets_total\n",
    "\n",
    "    # Probabilities of the word not being in a message\n",
    "    p_abs_word = (tweets_total - wc1) / tweets_total\n",
    "\n",
    "    # Calculation of the entropy of each word using IG\n",
    "    vocab_entropy[word] = IG(class_probs, [p_word, p_abs_word], [probs_1, probs_0])\n",
    "\n",
    "\n",
    "# They are sorted according to entropy value from highest to lowest\n",
    "vocab_entropy_ord = dict(sorted(vocab_entropy.items(), key=operator.itemgetter(1)))\n",
    "\n",
    "print(vocab_entropy_ord)\n",
    "\n",
    "wordVocab = []\n",
    "for word in vocab_entropy_ord:\n",
    "    wordVocab.append(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztiE2UDrgYIc"
   },
   "source": [
    "**One Hot Encoding shall be performed on the `Emocion` column. This technique, encodes categorical features as a single-use numeric array. The input to this transformer must be an array of integers or text strings, denoting the values taken by categorical (discrete) features. A binary column is created for each category and returns a sparse matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "hVAKU_iZf3xR",
    "outputId": "5aa0cc86-dd4d-413c-c9fd-1c59f84a7bed"
   },
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(dataset[\"Emocion\"])\n",
    "dataset.drop(['Emocion'], axis = 1, inplace = True)\n",
    "dataset = pd.concat([dataset, one_hot], axis = 1)\n",
    "dataset\n",
    "\n",
    "#Label Emocion is Emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWXs7qXHgjVU"
   },
   "source": [
    "# 4. Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYKYlRBIgqdr"
   },
   "source": [
    "**The dataset is divided in two, 30% for testing and 70% for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ike40xgIf31t"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "X = dataset['Id'].values\n",
    "\n",
    "y = dataset.iloc[:, -6:].values # with this I store the last 7 columns of the dataset (emotions) in the variable \"y\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfpnL8IxCZt7",
    "outputId": "741b93e9-526f-4c1d-86fe-2c86da3e4add"
   },
   "outputs": [],
   "source": [
    "num_ones = np.sum(y_train, axis=0)\n",
    "print(\"The following number of samples is taken from each class:\")\n",
    "print(\"anger, disgust, fear, joy, others, sadness\")\n",
    "print(num_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz-wCbLuyvfs"
   },
   "source": [
    "**Creation of the Corpus vocabulary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2hQtvXmhLUD",
    "outputId": "8fb62fc4-d8a3-4e4d-cc8e-ca3020b10b39"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "#oov_token is a special token in case this word is not in the dictionary\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "\n",
    "# The dictionary is created from the best \"X\" words (with the highest IG)\n",
    "tokenizer.fit_on_texts(wordVocab[:2155])\n",
    "\n",
    "# The following line of code is with the total dictionary (all words)\n",
    "#tokenizer.fit_on_texts(X_train.tolist()) \n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "maxlen = 33\n",
    "\n",
    "# We add \"0\" so that all input tensors have the same length, they are parsed to the length of maximum\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aq9boGkK9fR8",
    "outputId": "92d6f4b9-cf93-4c89-fe85-3c0a2b037f99"
   },
   "outputs": [],
   "source": [
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njnKJfeEhxsk"
   },
   "source": [
    "# 5. Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gEG0avOh5eL"
   },
   "source": [
    "The *TensorFlow* and the *Keras* library will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmoQUjCRh1Xk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg5pegmjiMnU"
   },
   "source": [
    "For the model, the simplest layers (`Dense`) will be used, on them, the activation function *ReLU* will be applied for the first three, and for the last one, *Softmax*.\n",
    "\n",
    "**ReLU activation function:** The *rectified linear unit activation function (*ReLU*) is the most widely used activation function for deep learning applications with the most successful and widely used results. The *ReLU* function represents a quasi-linear function and therefore retains the properties of linear models, with gradient descent methods.\n",
    "The activation function of *ReLU* performs a threshold operation for each input element where values less than 0 are set to 0, so the *ReLU* function is given by:\n",
    "\n",
    ">$f\\left ( x \\right ) = \\max\\left ( 0,x \\right ) = \\left\\{\\begin{matrix}\n",
    "x_{i}, & si \\; \\; x_{i} \\geq 0 \\\\ 0, & si \\; \\; x_{i} < 0\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "The main advantage of using *ReLU* in the calculation is that it guarantees a faster calculation, as no exponentials or divisions are calculated, with an overall improved calculation speed.\n",
    "\n",
    "**Softmax trigger function:** Used to calculate the probability distribution from a vector of real numbers. The *Softmax* function produces output in a range of values between 0 and 1, with the sum of the probabilities being equal to 1. The Softmax function is calculated using the ratio:\n",
    "\n",
    ">$f\\left ( x_{i} \\right ) = \\frac{exp\\left ( x_{i} \\right )}{\\sum_{j}^{exp\\left ( x_{j} \\right )}}$\n",
    "\n",
    "The Softmax function is used in multi-class models, where probabilities are returned for each class, with the target class having the highest probability. The Softmax function appears mainly in almost all output layers of deep learning architectures, where they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2D-fvJ4rJeJ"
   },
   "source": [
    "**Application of early-stop to avoid overfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdFlVuiljp_r"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtCLDtjC5f8s"
   },
   "source": [
    "**Cross Validation Hybrid Model**\n",
    "\n",
    "In this code only the training set is used to fit the model. The model is fitted using the model.fit(X[train], y[train]) function within the for train, test in kf.split(X) loop, where X[train] and y[train] are the training sets corresponding to each iteration of the loop. The test set X[test] and y[test] is used only to evaluate the performance of the model after each iteration of the cycle, using the function model.evaluate(X[test], y[test]).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAWVYlri2i-W",
    "outputId": "45e8090b-1496-40ed-a4bd-5f656556507f"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Bidirectional, GRU\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "filters = 180\n",
    "units = 96\n",
    "\n",
    "\n",
    "print(type_of_target(y))\n",
    "acc_per_fold=[]\n",
    "loss_per_fold=[]\n",
    "kf=KFold(n_splits=5, shuffle=True, random_state=999)\n",
    "\n",
    "cvscores=[]\n",
    "for train, test in kf.split(X_train, y_train):\n",
    "  model = Sequential()\n",
    "  embedding_layer = Embedding(vocab_size, 200, input_length=maxlen)\n",
    "\n",
    "  model.add(embedding_layer)\n",
    "\n",
    "  model.add(Conv1D(filters, 8, activation='relu'))\n",
    "\n",
    "  model.add(MaxPooling1D(10))\n",
    "\n",
    "  model.add(LSTM(units, dropout=0.2, recurrent_dropout=0.3))\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "\n",
    "  # More hidden layers can be added to test different configurations of the hybrid model\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "\n",
    "\n",
    "  # If we want 6 classes (number of emotions) we will use 6 neurons in the last dense layer.\n",
    "  model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "  model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "\n",
    "  early_stop = EarlyStopping(monitor = 'accuracy', mode = 'max', verbose = 1, patience = 5) # change the patience a bit to train the system more\n",
    "  model.fit(X[train], y[train], epochs=100, batch_size=256, verbose=1,validation_data = (X[test], y[test]), callbacks=[early_stop])\n",
    "\n",
    "  scores = model.evaluate(X[test], y[test], verbose=1)\n",
    "  print(f'Score for fold : {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "print(acc_per_fold)\n",
    "print(statistics.mean(acc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "QDXzYf-C1a-P",
    "outputId": "aaec59a3-93d8-4d2b-fbba-8b3e205ff3b7"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "instance = dataset[\"Id\"][40] # the message is arbitrarily chosen 40\n",
    "print(instance)\n",
    "\n",
    "\n",
    "instance = tokenizer.texts_to_sequences(instance)\n",
    "\n",
    "flat_list = [] # instance can have more than one sentence, it should be converted into a flat list\n",
    "for sublist in instance: # we go through each sub-list\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "flat_list = [flat_list]\n",
    "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "model.predict(instance) # we use the trained model to predict the instance class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1hCgwoOxVdj",
    "outputId": "9d2e327d-e43e-49ce-d44f-5928375cd2f8"
   },
   "outputs": [],
   "source": [
    "# We create the following code to insert a sentence and predict the emotion of the sentence according to the trained model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Label Amor/AdmiraciÃ³n is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label ComprensiÃ³n/EmpatÃ­a/IdentificaciÃ³n is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "classes=['AprobaciÃ³n/EmpatÃ­a/Confianza', 'DesinterÃ©s/Tedio', 'DecepciÃ³n/Tristeza', 'DesaprobaciÃ³n','Enfado/Ira','InterÃ©s/AnticipaciÃ³n/Hype','Indeterminado']\n",
    "\n",
    "new_text = input(\"Enter the new phrase to predict its emotion: \")\n",
    "preprocessed_text = [processing(new_text)]\n",
    "\n",
    "preprocessed_text = [word.lower() for word in preprocessed_text]\n",
    "\n",
    "if len(preprocessed_text) > maxlen:\n",
    "    preprocessed_text = preprocessed_text[:maxlen]\n",
    "\n",
    "# Convert pre-processed text into a sequence of words\n",
    "new_text_sequence = tokenizer.texts_to_sequences(preprocessed_text)\n",
    "# Apply padding to the sequence so that it has the same length.\n",
    "new_text_padded = pad_sequences(new_text_sequence, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Making the prediction\n",
    "prediction = model.predict(new_text_padded)\n",
    "class_index = np.argmax(prediction)\n",
    "class_label = classes[class_index]\n",
    "print(\"The predicted class for the sentence is:\", class_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-ggsUKOj9uh"
   },
   "source": [
    "#6. Evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHng55KckEw0"
   },
   "source": [
    "**A *dataframe* is to be created containing the values obtained by *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi76uc04kFUs"
   },
   "outputs": [],
   "source": [
    "df_model = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "11Fx33x6kFXK",
    "outputId": "5b1ce01e-b239-4fd0-a127-1bf8040339c5"
   },
   "outputs": [],
   "source": [
    "df_model['Epoch'] = range(1, df_model.shape[0] + 1)\n",
    "df_model.index = df_model['Epoch']\n",
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f7kQztrkLE7"
   },
   "source": [
    "**The accuracy (accuracy) of the model is then calculated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QR6rhbvkFZS",
    "outputId": "279657d9-49af-4d39-f626-7fec463b1184"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size = 32, verbose = 1)\n",
    "\n",
    "print('\\nAccuracy - Data Test:', round(score[1], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEXGH7YEkdOy"
   },
   "source": [
    "**Finally, the predictions will be calculated to compute the confusion matrix of the model (`confusion_matrix`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "9s9yl67QkkBS",
    "outputId": "594746a0-5e99-482b-c0ff-ea8d95d8b9bf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "prediccion = model.predict(X_test)\n",
    "confusionMatrix = confusion_matrix(np.argmax(y_test, axis = 1), np.argmax(prediccion, axis = 1))\n",
    "df_confusionMatrix = pd.DataFrame(confusionMatrix,\n",
    "                                   index = dataset.columns[-6:],\n",
    "                                   columns = dataset.columns[-6:])\n",
    "plt.figure(figsize = (6, 4))\n",
    "sns.heatmap(df_confusionMatrix, annot = True,fmt='g', annot_kws={\"size\": 14}, cmap = 'BuPu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OV6IoPLknGa"
   },
   "source": [
    "**A report showing the main metrics of the classification (classification_report) is also created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3XgvL2hkmg7",
    "outputId": "3153ea85-e4a2-40f9-b13d-e01acd49f6c6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "emotions = dataset.columns[-6:]\n",
    "print(classification_report(np.argmax(y_test, axis = 1), np.argmax(prediccion, axis = 1), target_names=emotions))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
