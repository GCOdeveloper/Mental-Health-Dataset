{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6F7sYiTedSz"
   },
   "source": [
    "# 1. Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22v8oPQVeLJV"
   },
   "source": [
    "**Import some libraries and the stored Corpus (dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMDT_JUiZ98-",
    "outputId": "c745f691-8445-4733-a640-b9c994569ef2"
   },
   "outputs": [],
   "source": [
    "#Import certain libraries and mount the drive where the corpus is stored\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style(style = 'whitegrid')\n",
    "%matplotlib inline\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ycOQtTJfkDq"
   },
   "source": [
    "#2. Analysis of the Mental Health dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "6SdIn55UeVGj",
    "outputId": "283cc70c-d097-47af-b0ac-eba148eaba6a"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/content/drive/MyDrive/DatasetMH_Emotions.csv\", sep = \";\", header = None)\n",
    "#Remove the first line\n",
    "dataset.drop(dataset.head(1).index,inplace = True)\n",
    "\n",
    "dataset.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60MGSE8qxeU8"
   },
   "source": [
    "**Columns are renamed to organise tweets with corresponding emotion**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDxY3YYmeweV"
   },
   "outputs": [],
   "source": [
    "dataset.columns = [\"Id\", \"Emoticos\", \"Polaridad\",\"Emocion\", \"nada\", \"nada\", \"nada\"]\n",
    "\n",
    "#Label Polaridad is Polarity\n",
    "#Label Emoticonos is Emoticons\n",
    "#Label nada is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "KLZ3vR-iAnQG",
    "outputId": "4745ee71-b3b4-4773-e6bd-725edd46b94c"
   },
   "outputs": [],
   "source": [
    "dataset.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mN1UFiWxi6A"
   },
   "source": [
    "**Graph to show the distribution of Instagram comments in the dataset by emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "id": "cK4D-GFse3Zl",
    "outputId": "520f516b-d04b-4b15-f01b-7a5b8df64503"
   },
   "outputs": [],
   "source": [
    "#Label Positiva is Positive\n",
    "#Label Negativa is Negative\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "#Label Amor/Admiración is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label Comprensión/Empatía/Identificación is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.countplot(x = 'Emocion', data = dataset, palette = 'rocket',\n",
    "\n",
    "              order=['Amor/Admiración', 'Gratitud', 'Comprensión/Empatía/Identificación', 'Tristeza/Pena','Enfado/Desprecio/Burla','Indeterminado']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUXEDGeox1_q"
   },
   "source": [
    "**6 datasets are created, one for each class to print the number of tweets of each class, in this case of each emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2VMqXAte4bb"
   },
   "outputs": [],
   "source": [
    "dataset_Love = dataset[dataset['Emocion'] == 'Amor/Admiración']\n",
    "dataset_Gratitude = dataset[dataset['Emocion'] == 'Gratitud']\n",
    "dataset_Empathy = dataset[dataset['Emocion'] == 'Comprensión/Empatía/Identificación']\n",
    "dataset_Sad = dataset[dataset['Emocion'] == 'Tristeza/Pena']\n",
    "dataset_Anger = dataset[dataset['Emocion'] == 'Enfado/Desprecio/Burla']\n",
    "dataset_Neutral = dataset[dataset['Emocion'] == 'Indeterminado']\n",
    "\n",
    "#Label Amor/Admiración is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label Comprensión/Empatía/Identificación is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvTCn0DTx1Mr"
   },
   "source": [
    "**Dataset visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "YfcB_kage4jT",
    "outputId": "1c016e18-04e0-4e69-997b-5a217243e938"
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset_Love, dataset_Gratitude, dataset_Empathy,\n",
    "                     dataset_Sad, dataset_Anger, dataset_Neutral], axis = 0)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soQ1bSLnyazT"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArOmSkXwf3a8",
    "outputId": "ccca2d89-f368-43b6-f300-a9920d2272ac"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph8kFdvCgGEr"
   },
   "source": [
    "# 3. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3UBKGLdyfUa"
   },
   "source": [
    "**Text pre-processing and tokenisation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sNguvKNf3ia"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk import TweetTokenizer\n",
    "import spacy\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# We create the function 'processed' which will delete stopwords and some characters peculiar to social networks\n",
    "def processing(text):\n",
    "  #Spanish stopwords \n",
    "  stopWords_without_prepositions = {'al', 'algo', 'algunas', 'algunos', 'antes', 'como', 'cual', 'cuando', 'del', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
    "                               'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve',\n",
    "                               'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviéramos', 'estuviésemos', 'estuvo', 'está', 'estábamos', 'estáis', 'están', 'estás', 'esté', 'estéis',\n",
    "                               'estén', 'estés', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuéramos', 'fuésemos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrá', 'habrán', 'habrás',\n",
    "                               'habré', 'habréis', 'habría', 'habríais', 'habríamos', 'habrían', 'habrías', 'habéis', 'había', 'habíais', 'habíamos', 'habían', 'habías', 'han', 'has', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayáis', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras',\n",
    "                               'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiéramos', 'hubiésemos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'más', 'mí', 'mía', 'mías', 'mío', 'míos', 'nada', 'nos',\n",
    "                               'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'pero', 'poco', 'porque', 'que', 'quien', 'quienes', 'qué', 'se', 'sea', 'seamos', 'sean', 'seas', 'sentid', 'sentida', 'sentidas', 'sentido', 'sentidos', 'seremos',\n",
    "                               'será', 'serán', 'serás', 'seré', 'seréis', 'sería', 'seríais', 'seríamos', 'serían', 'serías', 'seáis', 'siente', 'sintiendo', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sí', 'también', 'tanto', 'te', 'tendremos', 'tendrá', 'tendrán', 'tendrás',\n",
    "                               'tendré', 'tendréis', 'tendría', 'tendríais', 'tendríamos', 'tendrían', 'tendrías', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengáis', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenéis', 'tenía', 'teníais', 'teníamos', 'tenían', 'tenías',\n",
    "                               'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviéramos', 'tuviésemos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos',\n",
    "                               'tú', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos'}\n",
    "  #Translation into english  of stopwords\n",
    "  #stopWords_without_prepositions = {\"'to', 'something', 'some', 'some', 'before', 'like', 'which', 'when', 'of the', 'where', 'during', 'and', 'the', 'she', 'they', 'they', 'he', 'were', 'were', 'were', 'were', 'is', 'that', 'those', 'that', 'that', 'those', 'this', 'was', 'were', 'were', 'were', 'you were', 'be', 'we will', 'will be', 'will be', 'will be', 'will be', 'I will be', 'will be', 'would be', 'would be', 'would be', 'would be', 'would be', 'these', 'this', 'let us', 'this', 'these', 'I am', 'I was', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'was', 'were', 'were', 'were', 'were', 'were', 'was', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were',}\n",
    "\n",
    "\n",
    "  DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]\n",
    "\n",
    "    \n",
    "  SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
    "           ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),\n",
    "           ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]\n",
    "  #Translation into english  \n",
    "  #SLANG = [(‘d’,‘of’), (‘[qk]’,‘what’), (‘xo’,‘but’), (‘xa’,‘for’), (‘[xp]q’,‘why’),(‘es[qk]’,‘is that’),\n",
    "           #(‘fvr’, ‘please’),(‘(xfa|xf|pf|plis|pls|please)’, ‘please’), (‘dnd’, ‘where’), (‘tb’, ‘also’),\n",
    "           #(‘(tq|tk)’, ‘I love you’), (‘(tqm|tkm)’, ‘I love you very much’), (‘x’, ‘for’), (‘+’, ‘more’)]\n",
    "\n",
    "  # Delete mentions  @, # , links...\n",
    "  text = str(text)\n",
    "  text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
    "  text = re.sub(r'RT[|\\s]', ' ', text)\n",
    "  text = re.sub(r'#', ' ', text)\n",
    "  text = re.sub(r'https?:\\/\\/\\S+', ' ', text)\n",
    "\n",
    "  stemming = True\n",
    "\n",
    "  lemmatization = False\n",
    "\n",
    "  #Stemming in spanish\n",
    "  _stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "  #lemmatisation in Spanish\n",
    "  #nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "  # Message tokeniser (we use this one from Twitter)\n",
    "  _tokenizer = TweetTokenizer().tokenize\n",
    "\n",
    "  _stemming = stemming\n",
    "\n",
    "\n",
    "  # Convert to lower case\n",
    "  text = text.lower()\n",
    "\n",
    "\n",
    "  # Delete numbers and carriage returns\n",
    "  text = re.sub(r'(\\d+|\\n)', '', text)\n",
    "\n",
    "  # Deleting vowels with diacritical marks\n",
    "  for s,t in DIACRITICAL_VOWELS:\n",
    "    text = re.sub(r'{0}'.format(s), t, text)\n",
    "\n",
    "  # Delete repeated characters\n",
    "  text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "  # Normalise laughter and replace with predefined variables\n",
    "  text = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', ' ', text)\n",
    "  text = re.sub(r'\\b(juas+|lol)\\b', ' ', text)\n",
    "\n",
    "  # translate slang\n",
    "  for s,t in SLANG:\n",
    "    text = re.sub(r'\\b{0}\\b'.format(s), t, text)\n",
    "\n",
    "\n",
    "  pattern = r'''(?x)                  # Set flag to allow verbose regexps\n",
    "              (?:[A-Z]\\.)+            # Abbreviations, e.g. U.S.A \n",
    "              | \\w+(?:-\\w+)*          # Words with optional internal hyphens\n",
    "              | \\$?\\d+(?:\\.\\d+)?%?    # Currency and precentages, e.g. $12.40 82% \n",
    "              | \\.\\.\\.                # Ellipsis \n",
    "              | [][.,;\"'?():-_`]      # These are separate tokens; includes \n",
    "              | [😀\\😁\\😂\\🤣\\😃\\😄\\😅\\😆\\😉\\😊\\😋\\😎\\😍\\😘\\😗\\😙\\😚\\☺\\🙂\\🤗\\🤩\\😌\\😛\\😜\\😝\\🤤\\🤑\\😇\\🤭\\😺\\😸\\😹\\😻\\😽\\💪\\✌\\🖐\\✋\\👌\\👍\\👋\\👏\\🙌\\🙏\\💋\\💘\\❤\\💓\\💔\\💕\\💖\\💗\\💙\\💚\\💛\\🧡\\💜\\🖤\\💝\\💞\\💟\\❣\\💌\\🍺\\🍻\\🎉\\🎊\\🙋\\🕺\\💃] # \\:d\\:)\\:-)\\:-d\\;d\\;-)\\=d\\;)\\:]\\:-]\\=)\\=]\\(:\\xd\\:p\\:-p\\8)\\xp\\<3\n",
    "              | [:d]\n",
    "              | [:)]\n",
    "              #| [:-)]\n",
    "              | [:-d]\n",
    "              | [;d]\n",
    "              #| [;-)]\n",
    "              | [=d]\n",
    "              | [;)]\n",
    "              | [:]]\n",
    "              | [:-]]\n",
    "              | [=)]\n",
    "              | [=]]\n",
    "              | [(:]\n",
    "              | [xd]\n",
    "              | [:p]\n",
    "              | [:-p]\n",
    "              | [8)]\n",
    "              | [xp]\n",
    "              | [<3]\n",
    "              | [🤔\\🤨\\😐\\😑\\😶\\🙄\\😏\\😮\\🤐\\😯\\😒\\😕\\🙃\\😲\\😼\\🤷] # \\:-|\\:|]\n",
    "              | [:-|]\n",
    "              | [:|]\n",
    "              | [😣\\😥\\😪\\😫\\😓\\😔\\☹\\🙁\\😖\\😞\\😟\\😤\\😢\\😭\\😦\\😧\\😨\\😩\\🤯\\😬\\😰\\😱\\😳\\😵\\😡\\😠\\🤬\\😷\\🤒\\🤕\\🤢\\🤮\\🤧\\💩\\🙀\\😿\\😾\\🖕\\👎\\⛔\\🚫\\🤦] # \\:-(\\:(\\:-<\\:<\\:-[\\:[\\>:-[\\>:[\\:-{\\:{\\:-@\\:@\\>:-(\\>:(\\:-(\\:(\\d:\\:\\\\:/\\:-/\\:-\\\\dx\\d8\n",
    "              #| [:-(]\n",
    "              #| [:(]\n",
    "              | [:-<]\n",
    "              | [:<]\n",
    "              | [:-[]\n",
    "              | [:[]\n",
    "              | [>:-[]\n",
    "              | [>:[]\n",
    "              | [:-{]\n",
    "              | [:{]\n",
    "              | [:-@]\n",
    "              | [:@]\n",
    "              #| [>:-(]\n",
    "              | [>:(]\n",
    "              | [:'-(]\n",
    "              | [:'(]\n",
    "              | [d:]\n",
    "              | [:\\]\n",
    "              | [:/]\n",
    "              #| [:-/]\n",
    "              | [:-\\]\n",
    "              | [dx]\n",
    "              | [d8]\n",
    "              '''\n",
    "\n",
    "  if _stemming:\n",
    "    text = ' '.join(_stemmer.stem(w) for w in _tokenizer(text))\n",
    "\n",
    "  if lemmatization:\n",
    "    text_aux=nlp(text)\n",
    "    for word in text_aux:\n",
    "      text+=str(word.lemma_)+\" \"\n",
    "\n",
    "\n",
    "  words = nltk.regexp_tokenize(text, pattern)\n",
    "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))      # Remove punctuation marks\n",
    "\n",
    "  stripped = [re_punc.sub('', w) for w in words]                    # Remove stopwords\n",
    "\n",
    "\n",
    "  text = [w for w in stripped if  w.lower() not in stopWords_without_prepositions]\n",
    "\n",
    "\n",
    "  return (\" \".join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7HCTeRxgN1M"
   },
   "source": [
    "**The processed function is applied to each Instagram comment in the corpus. We process the data and apply stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JuV8R9Tf3sC",
    "outputId": "adf7d5f2-6dd1-4b60-c04a-d7c6c2121fce"
   },
   "outputs": [],
   "source": [
    "dataset['Id'] = dataset['Id'].apply(processing)\n",
    "dataset['Id'] = dataset['Id'].str.lower()\n",
    "dataset[\"Id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07ZxNtn3yzSy"
   },
   "source": [
    "**Create a WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "Gks181obEfAy",
    "outputId": "a559990f-00b7-4447-bb5a-3fcfcad21d84"
   },
   "outputs": [],
   "source": [
    "#wordCloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "dataset_A = dataset[dataset['Emocion'] == 'Amor/Admiración']\n",
    "dataset_D = dataset[dataset['Emocion'] == 'Gratitud']\n",
    "dataset_F = dataset[dataset['Emocion'] == 'Comprensión/Empatía/Identificación']\n",
    "dataset_J = dataset[dataset['Emocion'] == 'Tristeza/Pena']\n",
    "dataset_SAD = dataset[dataset['Emocion'] == 'Enfado/Desprecio/Burla']\n",
    "dataset_OTHERS = dataset[dataset['Emocion'] == 'Indeterminado']\n",
    "\n",
    "#Label Emocion is Emotion\n",
    "\n",
    "#Label Amor/Admiración is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label Comprensión/Empatía/Identificación is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "tweets= dataset_J[\"Id\"].head(70).values\n",
    "tweets=str(tweets)\n",
    "print(len(tweets))\n",
    "stop_words_sp = set(stopwords.words('spanish'))\n",
    "wordcloudimage = WordCloud(\n",
    "                          max_words=50,\n",
    "                          max_font_size=500,\n",
    "                          font_step=2,\n",
    "                          stopwords=stop_words_sp,\n",
    "                          background_color='white',\n",
    "                          width=1000,\n",
    "                          height=720\n",
    "                          ).generate(tweets)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloudimage)\n",
    "wordcloudimage\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfMSUaY3CeY0"
   },
   "source": [
    "**Vocabulary building with 6 classes (emotions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6WMjRlDn7Dm",
    "outputId": "fa08c717-4ca8-4d4a-ed30-455ded13035f"
   },
   "outputs": [],
   "source": [
    "#Label Amor/Admiración is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label Comprensión/Empatía/Identificación is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "vocab={}\n",
    "labels=['Amor/Admiración','Gratitud', 'Comprensión/Empatía/Identificación', 'Tristeza/Pena', 'Enfado/Desprecio/Burla','Indeterminado']\n",
    "numero_classes=len(labels)\n",
    "_tokenizer = TweetTokenizer().tokenize\n",
    "\n",
    "for i in dataset.index:\n",
    "  emotion=dataset[\"Emocion\"][i]\n",
    "  if emocion ==\"Amor/Admiración\":\n",
    "    position=0\n",
    "  elif emocion == \"Gratitud\":\n",
    "    position=1\n",
    "  elif emocion == \"Comprensión/Empatía/Identificación\":\n",
    "    position=2\n",
    "  elif emocion == \"Tristeza/Pena\":\n",
    "    position=3\n",
    "  elif emocion == \"Enfado/Desprecio/Burla\":\n",
    "    position=4\n",
    "  else:\n",
    "    position=5\n",
    "  for word in _tokenizer(dataset[\"Id\"][i]):\n",
    "    if word not in vocab.keys():\n",
    "      vocab[word]=[0] * 6 # with this I turn it into a list of 7 integers\n",
    "      vocab[word][position] = 1\n",
    "    else:\n",
    "      vocab[word][position] += 1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqAMAb5HCluZ"
   },
   "source": [
    "**Entropy and information gain formulas for selecting the optimal vocabulary for the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swFqxKxjeLI-"
   },
   "outputs": [],
   "source": [
    "def entropy(probs, adjust=1e-15):\n",
    "  total=0\n",
    "  for prob in probs:\n",
    "    if(prob>0):\n",
    "      total+= (prob + adjust) * np.math.log(prob+adjust,2)\n",
    "\n",
    "  return total\n",
    "\n",
    "\n",
    "def IG(corpus_probs, word_weigths, word_probs):\n",
    "\n",
    "  corpus_entropy= entropy(corpus_probs)\n",
    "  word_entropy=0\n",
    "\n",
    "  for i in range(len(word_weigths)):\n",
    "    word_entropy+= (word_weigths[i]* entropy(word_probs[i]))\n",
    "  return corpus_entropy - word_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfAAt5zoC97B"
   },
   "source": [
    "**Calculation of the IG of each word in the corpus for the 6 classes that are emotions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-n7JbAS46H2Z",
    "outputId": "b8be4e5e-a58f-44f2-c4a0-87b791a39a7f"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "tweets_Love = len(dataset_Love)\n",
    "tweets_Gratitude = len(dataset_Gratitude)\n",
    "tweets_Empathy = len(dataset_Empathy)\n",
    "tweets_Sad = len(dataset_Sad)\n",
    "tweets_Anger = len(dataset_Anger)\n",
    "tweets_Neutral = len(dataset_Neutral)\n",
    "tweets_total =tweets_Love + tweets_Gratitude + tweets_Empathy + tweets_Sad + tweets_Anger + tweets_Neutral\n",
    "\n",
    "class_counts = [ tweets_Love, tweets_Gratitude, tweets_Empathy, tweets_Sad, tweets_Anger,\n",
    "                tweets_Neutral]\n",
    "class_probs = np.array(class_counts) / tweets_total\n",
    "\n",
    "vocab_entropy = {}\n",
    "for word in vocab.keys():\n",
    "\n",
    "    # Frequency of the word in the corpus\n",
    "    wc1 = sum(vocab[word])\n",
    "\n",
    "    # Frequency of the word not being in the corpus\n",
    "    wc0 = tweets_total - wc1\n",
    "\n",
    "    # Probabilities of the word to be in each of the classes\n",
    "    probs_1 = [vocab[word][i] / wc1 for i in range(len(vocab[word]))]\n",
    "\n",
    "    # Probabilities of the word not being in each of the classes\n",
    "    probs_0 = [(class_counts[i] - vocab[word][i]) / (tweets_total - wc1) for i in range(len(vocab[word]))]\n",
    "\n",
    "    # Probabilities of the word being in a message\n",
    "    p_word = wc1 / tweets_total\n",
    "\n",
    "    # Probability that the word is not in a message\n",
    "    p_abs_word = (tweets_total - wc1) / tweets_total\n",
    "\n",
    "    # Calculation of the entropy of each word using IG\n",
    "    vocab_entropy[word] = IG(class_probs, [p_word, p_abs_word], [probs_1, probs_0])\n",
    "\n",
    "\n",
    "# vocab_entropy_ord = dict(sorted(vocab_entropy.items(), key=operator.itemgetter(1), reverse=True))\n",
    "# The words are sorted according to the IG value, from the highest value to the lowest value\n",
    "vocab_entropy_ord = dict(sorted(vocab_entropy.items(), key=operator.itemgetter(1)))\n",
    "\n",
    "print(vocab_entropy_ord)\n",
    "\n",
    "wordsVocab = []\n",
    "for word in vocab_entropy_ord:\n",
    "    wordsVocab.append(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztiE2UDrgYIc"
   },
   "source": [
    "**One Hot Encoding technique shall be performed on the `Emocion` column. This technique, encodes categorical features as a single-use numeric array. The input to this transformer must be an array of integers or text strings, denoting the values taken by categorical (discrete) features. A binary column is created for each category and a sparse matrix is returned.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "hVAKU_iZf3xR",
    "outputId": "f1633845-cccf-4768-fa9d-f6a1bc4e825f"
   },
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(dataset[\"Emocion\"])\n",
    "dataset.drop(['Emocion'], axis = 1, inplace = True)\n",
    "dataset = pd.concat([dataset, one_hot], axis = 1)\n",
    "dataset\n",
    "\n",
    "#Label Emocion is Emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWXs7qXHgjVU"
   },
   "source": [
    "# 4. Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYKYlRBIgqdr"
   },
   "source": [
    "**The dataset is divided in two, 30% for testing and 70% for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ike40xgIf31t"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "X = dataset['Id'].values\n",
    "\n",
    "y = dataset.iloc[:, -6:].values # with this I store the last 7 columns of the dataset (the emotions) in the variable \"y\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfpnL8IxCZt7",
    "outputId": "9f414b83-cead-437e-b597-855937e4eb2a"
   },
   "outputs": [],
   "source": [
    "num_ones = np.sum(y_train, axis=0)\n",
    "print(\"The following number of samples is taken from each class:\")\n",
    "print(\"anger, disgust, fear, joy, others, sadness\")\n",
    "print(num_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz-wCbLuyvfs"
   },
   "source": [
    "**Creation of the Corpus vocabulary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2hQtvXmhLUD",
    "outputId": "b570e709-1243-4013-bd54-e84fad96fec1"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences \n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "# oov_token is a special token in case this word is not in the dictionary\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "\n",
    "# The dictionary is created from the best \"X\" number of words\n",
    "tokenizer.fit_on_texts(wordVocab[:1650])\n",
    "\n",
    "# the following line of code is the total dictionary without the \"X\" best words\n",
    "# to take all vocabulary words, but select \"#tokenizer.fit_on_texts(wordsVocab[:1651])\" and enter the desired number of words, in this case 1651\n",
    "#tokenizer.fit_on_texts(X_train.tolist()) # the tolist is added because it cannot be done with arrays\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "maxlen = 33\n",
    "\n",
    "# We add \"0\" so that all input tensors have the same length, they are parsed to the length of maximum\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aq9boGkK9fR8",
    "outputId": "72cbdac4-8ca9-4045-b126-55eb37246160"
   },
   "outputs": [],
   "source": [
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njnKJfeEhxsk"
   },
   "source": [
    "# 5. Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oprjlA_AM4vF"
   },
   "source": [
    "\n",
    "### Adaptation to RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dC_j4pdM56a",
    "outputId": "eef82b88-e707-4900-d526-9512a28d41d8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## This block is to move y_train and y_test from a 6-column form to a 1-column form.\n",
    "y_test_def = np.empty(y_test.shape[0], dtype=object)\n",
    "\n",
    "#Label Amor/Admiración is Love/admiration\n",
    "#Label Gratitud is Gratitude\n",
    "#Label Tristeza/Pena is Sadness\n",
    "#Label Enfado/Desprecio/Burla is Anger/contempt/mockery\n",
    "#Label Comprensión/Empatía/Identificación is Comprehension/empathy/identification\n",
    "#Label Indeterminado is Neutral\n",
    "\n",
    "# Iteration for each row of y_test\n",
    "for i in range(y_test.shape[0]):\n",
    "    # I get the values of the current row\n",
    "    row_values = y_test[i]\n",
    "\n",
    "    # I check the conditions and assign the labels\n",
    "    if row_values[0] == 1:\n",
    "        y_test_def[i] = 'Amor/Admiración'\n",
    "    elif row_values[1] == 1:\n",
    "        y_test_def[i] = 'Comprensión/Empatía/Identificación'\n",
    "    elif row_values[2] == 1:\n",
    "        y_test_def[i] = 'Enfado/Desprecio/Burla'\n",
    "    elif row_values[3] == 1:\n",
    "        y_test_def[i] = 'Gratitud'\n",
    "    elif row_values[4] == 1:\n",
    "        y_test_def[i] = 'Indeterminado'\n",
    "    elif row_values[5] == 1:\n",
    "        y_test_def[i] = 'Tristeza/Pena'\n",
    "\n",
    "\n",
    "# I check the values of y_test_def\n",
    "y_test_def.shape\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# I create an empty array to store the modified values of y_test\n",
    "y_train_def = np.empty(y_train.shape[0], dtype=object)\n",
    "\n",
    "# Iteration for each row of y_test\n",
    "for i in range(y_train.shape[0]):\n",
    "    # I get the values of the current row\n",
    "    row_values = y_train[i]\n",
    "\n",
    "    # I check the conditions and assign the labels\n",
    "    if row_values[0] == 1:\n",
    "        y_train_def[i] = 'Amor/Admiración'\n",
    "    elif row_values[1] == 1:\n",
    "       y_train_def[i] = 'Comprensión/Empatía/Identificación'\n",
    "    elif row_values[2] == 1:\n",
    "        y_train_def[i] = 'Enfado/Desprecio/Burla'\n",
    "    elif row_values[3] == 1:\n",
    "        y_train_def[i] = 'Gratitud'\n",
    "    elif row_values[4] == 1:\n",
    "        y_train_def[i] = 'Indeterminado'\n",
    "    elif row_values[5] == 1:\n",
    "        y_train_def[i] = 'Tristeza/Pena'\n",
    "\n",
    "\n",
    "# I check the values of y_test_def\n",
    "y_train_def.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXImIHB9Fbyg"
   },
   "source": [
    "### RANDOM_FOREST algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNPBooXiNEq1",
    "outputId": "c51e8c34-570a-429c-f4fd-eda94bf17231"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "precision_per_class = []\n",
    "recall_per_class = []\n",
    "f1_per_class = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=47)\n",
    "\n",
    "\n",
    "for train, test in kf.split(X_train):\n",
    "    model1 = RandomForestClassifier(max_features=\"sqrt\",n_estimators=510) # you can modify the hyperparameters n_stimators and max_features\n",
    "\n",
    "    model1.fit(X_train, y_train_def)\n",
    "\n",
    "    y_pred_rf = model1.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test_def, y_pred_rf, average=None)\n",
    "    recall = recall_score(y_test_def, y_pred_rf, average=None)\n",
    "    f1 = f1_score(y_test_def, y_pred_rf, average=None)\n",
    "\n",
    "    precision_per_class.append(precision)\n",
    "    recall_per_class.append(recall)\n",
    "    f1_per_class.append(f1)\n",
    "\n",
    "precision_avg = np.mean(precision_per_class, axis=0)\n",
    "recall_avg = np.mean(recall_per_class, axis=0)\n",
    "f1_avg = np.mean(f1_per_class, axis=0)\n",
    "\n",
    "print(\"Precision:\", precision_avg)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Obtain the classification report\n",
    "classification_rep = classification_report(y_test_def, y_pred_rf)\n",
    "\n",
    "# Print the classification repor\n",
    "print(classification_rep)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test_def, y_pred_rf)\n",
    "\n",
    "# Print accuracy with decimals\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "H2ugZy1VNHHF",
    "outputId": "9a733c76-670a-457e-dcf3-c2043241f908"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion = confusion_matrix(y_test_def, y_pred_rf)\n",
    "\n",
    "# Obtain class labels\n",
    "labels = np.unique(y_test_def)\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(confusion, annot=True, cmap='Blues', fmt='d', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "\n",
    "# Configure labels and chart title\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Show the confusion matrix\n",
    "plt.show()\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "accuracy_per_class = np.diag(confusion) / confusion.sum(axis=1)\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Accuracy for each class {label}: {accuracy_per_class[i]}\")\n",
    "\n",
    "# Calculate the global accuracy\n",
    "total_accuracy = np.trace(confusion) / confusion.sum()\n",
    "print(f\"Global Accuracy: {total_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9Uyys12NLBw",
    "outputId": "a2050c0f-768b-4c0e-9031-351ee130749c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Obtain the classification report\n",
    "classification_rep = classification_report(y_test_def, y_pred_rf)\n",
    "\n",
    "# Print the ranking report\n",
    "print(classification_rep)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_def, y_pred_rf)\n",
    "\n",
    "# Print accuracy with decimals\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
