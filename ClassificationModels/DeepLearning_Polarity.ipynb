{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6F7sYiTedSz"
      },
      "source": [
        "# **1. Libraries**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22v8oPQVeLJV"
      },
      "source": [
        "**Import some libraries and the Corpus (Mental Health dataset) stored in the drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMDT_JUiZ98-"
      },
      "outputs": [],
      "source": [
        "#Importar ciertas librerias y montar el drive donde se guarda el corpus\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set_style(style = 'whitegrid')\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ycOQtTJfkDq"
      },
      "source": [
        "#**2. Analysis of the Mental Health Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SdIn55UeVGj"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/DatasetMH.csv\", sep = \";\", header = None)\n",
        "dataset.drop(dataset.head(1).index,inplace = True)\n",
        "\n",
        "#I remove the first line with the heading\n",
        "\n",
        "# Check whether the first column of the dataset has been removed\n",
        "\n",
        "dataset.head(6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60MGSE8qxeU8"
      },
      "source": [
        "**Columns are named to organise Instragram messages with their corresponding polarity (class o category)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDxY3YYmeweV"
      },
      "outputs": [],
      "source": [
        "dataset.columns = [\"Tweet\", \"emote\", \"Rating_Polarity\",\"emocion\",\"none\",\"none\",\"none\"]\n",
        "\n",
        "#Label Polaridad is Polarity\n",
        "#Label Emoticonos is Emoticons\n",
        "#Label nada is null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mN1UFiWxi6A"
      },
      "source": [
        "**Graph to show the distribution of messages in the dataset according to emotion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK4D-GFse3Zl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "sns.countplot(x = 'Rating_Polarity', data = dataset, palette = 'rocket',\n",
        "              order=['Negativa', 'Positiva', 'Indeterminado']);\n",
        "plt.show()\n",
        "\n",
        "#Label Positiva is Positive\n",
        "#Label Negativa is Negative\n",
        "#Label Indeterminado is Neutral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUXEDGeox1_q"
      },
      "source": [
        "**3 datasets are created, one for each class to print the number of messages of each class, in this case of each polarity (Positive, Negative, Neutral)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2VMqXAte4bb"
      },
      "outputs": [],
      "source": [
        "dataset_Positive = dataset[dataset['Rating_Polarity'] == 'Positiva']\n",
        "dataset_Negative = dataset[dataset['Rating_Polarity'] == 'Negativa']\n",
        "dataset_None = dataset[dataset['Rating_Polarity'] == 'Indeterminado']\n",
        "\n",
        "\n",
        "print(\"NUMBER OF COMMNETS:\\n\",\n",
        "      \"\\nPositives   \", len(dataset_Positive),\n",
        "      \"\\nNegatives   \", len(dataset_Negative),\n",
        "      \"\\nNeutral     \", len(dataset_None))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvTCn0DTx1Mr"
      },
      "source": [
        "**Dataset of Mental Health**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfcB_kage4jT"
      },
      "outputs": [],
      "source": [
        "dataset = pd.concat([dataset_Positive, dataset_Negative, dataset_None], axis = 0)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soQ1bSLnyazT"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArOmSkXwf3a8",
        "outputId": "8d6ac286-2dbc-4e1c-9863-78fcda9738fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph8kFdvCgGEr"
      },
      "source": [
        "# **3. Data pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3UBKGLdyfUa"
      },
      "source": [
        "**Pre-processing and tokenisation function of dataset messages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sNguvKNf3ia"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "from nltk import TweetTokenizer\n",
        "import spacy\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# We create the function 'processed' which will delete stopwords and some characters peculiar to social networks\n",
        "def processing(text):\n",
        "  #Spanish stopwords\n",
        "  stopWords_without_prepositions = {'al', 'algo', 'algunas', 'algunos', 'antes', 'como', 'cual', 'cuando', 'del', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "                               'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve',\n",
        "                               'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviéramos', 'estuviésemos', 'estuvo', 'está', 'estábamos', 'estáis', 'están', 'estás', 'esté', 'estéis',\n",
        "                               'estén', 'estés', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuéramos', 'fuésemos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrá', 'habrán', 'habrás',\n",
        "                               'habré', 'habréis', 'habría', 'habríais', 'habríamos', 'habrían', 'habrías', 'habéis', 'había', 'habíais', 'habíamos', 'habían', 'habías', 'han', 'has', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayáis', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras',\n",
        "                               'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiéramos', 'hubiésemos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'más', 'mí', 'mía', 'mías', 'mío', 'míos', 'nada', 'nos',\n",
        "                               'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'pero', 'poco', 'porque', 'que', 'quien', 'quienes', 'qué', 'se', 'sea', 'seamos', 'sean', 'seas', 'sentid', 'sentida', 'sentidas', 'sentido', 'sentidos', 'seremos',\n",
        "                               'será', 'serán', 'serás', 'seré', 'seréis', 'sería', 'seríais', 'seríamos', 'serían', 'serías', 'seáis', 'siente', 'sintiendo', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sí', 'también', 'tanto', 'te', 'tendremos', 'tendrá', 'tendrán', 'tendrás',\n",
        "                               'tendré', 'tendréis', 'tendría', 'tendríais', 'tendríamos', 'tendrían', 'tendrías', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengáis', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenéis', 'tenía', 'teníais', 'teníamos', 'tenían', 'tenías',\n",
        "                               'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviéramos', 'tuviésemos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos',\n",
        "                               'tú', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos'}\n",
        "  #Translation into english  of stopwords\n",
        "  #stopWords_without_prepositions = {\"'to', 'something', 'some', 'some', 'before', 'like', 'which', 'when', 'of the', 'where', 'during', 'and', 'the', 'she', 'they', 'they', 'he', 'were', 'were', 'were', 'were', 'is', 'that', 'those', 'that', 'that', 'those', 'this', 'was', 'were', 'were', 'were', 'you were', 'be', 'we will', 'will be', 'will be', 'will be', 'will be', 'I will be', 'will be', 'would be', 'would be', 'would be', 'would be', 'would be', 'these', 'this', 'let us', 'this', 'these', 'I am', 'I was', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'was', 'were', 'were', 'were', 'were', 'were', 'was', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were', 'were',}\n",
        "\n",
        "\n",
        "  DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]\n",
        "\n",
        "\n",
        "\n",
        "  SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "           ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),\n",
        "           ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]\n",
        "  #Translation into english o SLANG\n",
        "  #SLANG = [(‘d’,‘of’), (‘[qk]’,‘what’), (‘xo’,‘but’), (‘xa’,‘for’), (‘[xp]q’,‘why’),(‘es[qk]’,‘is that’),\n",
        "           #(‘fvr’, ‘please’),(‘(xfa|xf|pf|plis|pls|please)’, ‘please’), (‘dnd’, ‘where’), (‘tb’, ‘also’),\n",
        "           #(‘(tq|tk)’, ‘I love you’), (‘(tqm|tkm)’, ‘I love you very much’), (‘x’, ‘for’), (‘+’, ‘more’)]\n",
        "\n",
        "  # Delete mentions  @, # , links...\n",
        "  text = str(text)\n",
        "  text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n",
        "  text = re.sub(r'RT[|\\s]', ' ', text)\n",
        "  text = re.sub(r'#', ' ', text)\n",
        "  text = re.sub(r'https?:\\/\\/\\S+', ' ', text)\n",
        "\n",
        "  stemming = True\n",
        "\n",
        "  lemmatization = False\n",
        "\n",
        "  #Stemming in spanish\n",
        "  _stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "  #lemmatisation in Spanish\n",
        "  #nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "  # Message tokeniser (we use this one from Twitter)\n",
        "  _tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "  _stemming = stemming\n",
        "\n",
        "\n",
        "  # Convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "\n",
        "  # Delete numbers and carriage returns\n",
        "  text = re.sub(r'(\\d+|\\n)', '', text)\n",
        "\n",
        "  # Deleting vowels with diacritical marks\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    text = re.sub(r'{0}'.format(s), t, text)\n",
        "\n",
        "  # Delete repeated characters\n",
        "  text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "  # Normalise laughter and replace with predefined variables\n",
        "  text = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', ' ', text)\n",
        "  text = re.sub(r'\\b(juas+|lol)\\b', ' ', text)\n",
        "\n",
        "  # translate slang\n",
        "  for s,t in SLANG:\n",
        "    text = re.sub(r'\\b{0}\\b'.format(s), t, text)\n",
        "\n",
        "\n",
        "  pattern = r'''(?x)                  # Set flag to allow verbose regexps\n",
        "              (?:[A-Z]\\.)+            # Abbreviations, e.g. U.S.A\n",
        "              | \\w+(?:-\\w+)*          # Words with optional internal hyphens\n",
        "              | \\$?\\d+(?:\\.\\d+)?%?    # Currency and precentages, e.g. $12.40 82%\n",
        "              | \\.\\.\\.                # Ellipsis\n",
        "              | [][.,;\"'?():-_`]      # These are separate tokens; includes\n",
        "              | [😀\\😁\\😂\\🤣\\😃\\😄\\😅\\😆\\😉\\😊\\😋\\😎\\😍\\😘\\😗\\😙\\😚\\☺\\🙂\\🤗\\🤩\\😌\\😛\\😜\\😝\\🤤\\🤑\\😇\\🤭\\😺\\😸\\😹\\😻\\😽\\💪\\✌\\🖐\\✋\\👌\\👍\\👋\\👏\\🙌\\🙏\\💋\\💘\\❤\\💓\\💔\\💕\\💖\\💗\\💙\\💚\\💛\\🧡\\💜\\🖤\\💝\\💞\\💟\\❣\\💌\\🍺\\🍻\\🎉\\🎊\\🙋\\🕺\\💃] # \\:d\\:)\\:-)\\:-d\\;d\\;-)\\=d\\;)\\:]\\:-]\\=)\\=]\\(:\\xd\\:p\\:-p\\8)\\xp\\<3\n",
        "              | [:d]\n",
        "              | [:)]\n",
        "              #| [:-)]\n",
        "              | [:-d]\n",
        "              | [;d]\n",
        "              #| [;-)]\n",
        "              | [=d]\n",
        "              | [;)]\n",
        "              | [:]]\n",
        "              | [:-]]\n",
        "              | [=)]\n",
        "              | [=]]\n",
        "              | [(:]\n",
        "              | [xd]\n",
        "              | [:p]\n",
        "              | [:-p]\n",
        "              | [8)]\n",
        "              | [xp]\n",
        "              | [<3]\n",
        "              | [🤔\\🤨\\😐\\😑\\😶\\🙄\\😏\\😮\\🤐\\😯\\😒\\😕\\🙃\\😲\\😼\\🤷] # \\:-|\\:|]\n",
        "              | [:-|]\n",
        "              | [:|]\n",
        "              | [😣\\😥\\😪\\😫\\😓\\😔\\☹\\🙁\\😖\\😞\\😟\\😤\\😢\\😭\\😦\\😧\\😨\\😩\\🤯\\😬\\😰\\😱\\😳\\😵\\😡\\😠\\🤬\\😷\\🤒\\🤕\\🤢\\🤮\\🤧\\💩\\🙀\\😿\\😾\\🖕\\👎\\⛔\\🚫\\🤦] # \\:-(\\:(\\:-<\\:<\\:-[\\:[\\>:-[\\>:[\\:-{\\:{\\:-@\\:@\\>:-(\\>:(\\:-(\\:(\\d:\\:\\\\:/\\:-/\\:-\\\\dx\\d8\n",
        "              #| [:-(]\n",
        "              #| [:(]\n",
        "              | [:-<]\n",
        "              | [:<]\n",
        "              | [:-[]\n",
        "              | [:[]\n",
        "              | [>:-[]\n",
        "              | [>:[]\n",
        "              | [:-{]\n",
        "              | [:{]\n",
        "              | [:-@]\n",
        "              | [:@]\n",
        "              #| [>:-(]\n",
        "              | [>:(]\n",
        "              | [:'-(]\n",
        "              | [:'(]\n",
        "              | [d:]\n",
        "              | [:\\]\n",
        "              | [:/]\n",
        "              #| [:-/]\n",
        "              | [:-\\]\n",
        "              | [dx]\n",
        "              | [d8]\n",
        "              '''\n",
        "\n",
        "  if _stemming:\n",
        "    text = ' '.join(_stemmer.stem(w) for w in _tokenizer(text))\n",
        "\n",
        "  if lemmatization:\n",
        "    text_aux=nlp(text)\n",
        "    for word in text_aux:\n",
        "      text+=str(word.lemma_)+\" \"\n",
        "\n",
        "\n",
        "  words = nltk.regexp_tokenize(text, pattern)\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))      # Remove punctuation marks\n",
        "\n",
        "  stripped = [re_punc.sub('', w) for w in words]                    # Remove stopwords\n",
        "\n",
        "\n",
        "  text = [w for w in stripped if  w.lower() not in stopWords_without_prepositions]\n",
        "\n",
        "\n",
        "  return (\" \".join(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7HCTeRxgN1M"
      },
      "source": [
        "\n",
        "The processed function is applied to each Instagram comment in the corpus. We process the data and apply stemming **texto en negrita**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JuV8R9Tf3sC"
      },
      "outputs": [],
      "source": [
        "dataset['Tweet'] = dataset['Tweet'].apply(procesado)\n",
        "dataset['Tweet'] = dataset['Tweet'].str.lower()\n",
        "\n",
        "dataset[\"Tweet\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ZxNtn3yzSy"
      },
      "source": [
        "**Create the word cloud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gks181obEfAy"
      },
      "outputs": [],
      "source": [
        "#Create wordCloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "#Label Positiva is Positive\n",
        "#Label Negativa is Negative\n",
        "#Label Indeterminado is Neutral\n",
        "\n",
        "datasetP=dataset[dataset['Rating_Polarity'] == 'Positiva']\n",
        "datasetN=dataset[dataset['Rating_Polarity'] == 'Negativa']\n",
        "datasetNO=dataset[dataset['Rating_Polarity'] == 'Indeterminado']\n",
        "tweets= datasetP[\"Tweet\"].head(1000).values\n",
        "tweets=str(tweets)\n",
        "print(len(tweets))\n",
        "stop_words_sp = set(stopwords.words('spanish'))\n",
        "stop_words_sp.update([\"mas\",\"si\",\"dice\",\"hoy\",\"dia\"])\n",
        "wordcloudimage = WordCloud(\n",
        "                          max_words=100,\n",
        "                          max_font_size=500,\n",
        "                          font_step=2,\n",
        "                          stopwords=stop_words_sp,\n",
        "                          background_color='white',\n",
        "                          width=1000,\n",
        "                          height=720\n",
        "                          ).generate(tweets)\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(wordcloudimage)\n",
        "wordcloudimage\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcR16ttUCjMS"
      },
      "source": [
        "**Vocabulary creation with 3 classes (Positive, Negative, Neutral class)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FupFmztfCcVe"
      },
      "outputs": [],
      "source": [
        "vocab={}\n",
        "etiquetas=[\"Negativo\",\"Positivo\",\"indeterminado\"]\n",
        "numero_clases=len(etiquetas)\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "\n",
        "#Label Positiva is Positive\n",
        "#Label Negativa is Negative\n",
        "#Label Indeterminado is Neutral\n",
        "\n",
        "for i in dataset.index:\n",
        "  polaridad=dataset[\"Rating_Polarity\"][i]\n",
        "  if polaridad ==\"Negativo\":\n",
        "    posicion=0\n",
        "  elif polaridad == \"Positivo\":\n",
        "    posicion=1\n",
        "  else:\n",
        "    posicion=2\n",
        "  for word in _tokenizer(dataset[\"Tweet\"][i]):\n",
        "    if word not in vocab.keys():\n",
        "      vocab[word]=[0] * 3\n",
        "      vocab[word][posicion] = 1\n",
        "    else:\n",
        "      vocab[word][posicion] += 1\n",
        "\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqAMAb5HCluZ"
      },
      "source": [
        "**Entropy and information gain formulas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swFqxKxjeLI-"
      },
      "outputs": [],
      "source": [
        "def entropy(probs, adjust=1e-15):\n",
        "  total=0\n",
        "  for prob in probs:\n",
        "    if(prob>0):\n",
        "      total+= (prob + adjust) * np.math.log(prob+adjust,2)\n",
        "\n",
        "  return total\n",
        "\n",
        "\n",
        "def IG(corpus_probs, word_weigths, word_probs):\n",
        "\n",
        "  corpus_entropy= entropy(corpus_probs)\n",
        "  word_entropy=0\n",
        "\n",
        "  for i in range(len(word_weigths)):\n",
        "    #print(i)\n",
        "    word_entropy+= (word_weigths[i]* entropy(word_probs[i]))\n",
        "  return corpus_entropy - word_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umHnvu6_DE-g"
      },
      "source": [
        "**Calculation of the IG of each word in the corpus for the classes (6 emotions)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8l4jwMmDDrI"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "tweets_positivos=len(dataset_Positive)\n",
        "tweets_negativos=len(dataset_Negative)\n",
        "tweets_indeterminados=len(dataset_None)\n",
        "\n",
        "tweets_total=tweets_positivos+tweets_negativos+tweets_indeterminados\n",
        "\n",
        "class_counts=[tweets_negativos,tweets_positivos,tweets_indeterminados]\n",
        "class_probs=np.array(class_counts) / tweets_total\n",
        "vocab_entropy={}\n",
        "\n",
        "vocab_entropy = {}\n",
        "for word in vocab.keys():\n",
        "\n",
        "    # Frequency of the word in the corpus\n",
        "    wc1 = sum(vocab[word])\n",
        "\n",
        "    # Frequency of the word not being in the corpus\n",
        "    wc0 = tweets_total - wc1\n",
        "\n",
        "    # Probabilities of the word to be in each of the classes\n",
        "    probs_1 = [vocab[word][i] / wc1 for i in range(len(vocab[word]))]\n",
        "\n",
        "    # Probabilities of the word not being in each of the classes\n",
        "    probs_0 = [(class_counts[i] - vocab[word][i]) / (tweets_total - wc1) for i in range(len(vocab[word]))]\n",
        "\n",
        "    # Probabilities of the word being in a message\n",
        "    p_word = wc1 / tweets_total\n",
        "\n",
        "    # Probabilities of the word not being in a message\n",
        "    p_abs_word = (tweets_total - wc1) / tweets_total\n",
        "\n",
        "    # Calculation of the entropy of each word using IG\n",
        "    vocab_entropy[word] = IG(class_probs, [p_word, p_abs_word], [probs_1, probs_0])\n",
        "\n",
        "\n",
        "# They are sorted according to entropy value from highest to lowest\n",
        "vocab_entropy_ord = dict(sorted(vocab_entropy.items(), key=operator.itemgetter(1)))\n",
        "\n",
        "print(vocab_entropy_ord)\n",
        "\n",
        "wordVocab = []\n",
        "for word in vocab_entropy_ord:\n",
        "    wordVocab.append(word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztiE2UDrgYIc"
      },
      "source": [
        "**One Hot Encoding shall be performed on the \"Rating_Polarity\" column. This technique, encodes categorical features as a single-use numeric array. The input to this transformer must be an array of integers or text strings, denoting the values taken by categorical (discrete) features. A binary column is created for each category and returns a sparse matrix.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVAKU_iZf3xR"
      },
      "outputs": [],
      "source": [
        "one_hot = pd.get_dummies(dataset[\"Rating_Polarity\"])\n",
        "dataset.drop(['Rating_Polarity'], axis = 1, inplace = True)\n",
        "dataset = pd.concat([dataset, one_hot], axis = 1)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWXs7qXHgjVU"
      },
      "source": [
        "# 4. **Training and Test Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYKYlRBIgqdr"
      },
      "source": [
        "**The dataset is divided in two, 30% for testing and 70% for training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ike40xgIf31t"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "X = dataset['Tweet'].values\n",
        "y = dataset.iloc[:,-3:].values # with this I store the last 3 columns of the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "8MW5_X0AEOQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz-wCbLuyvfs"
      },
      "source": [
        "**Creation of the Corpus vocabulary**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2hQtvXmhLUD"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "#oov_token is a special token in case this word is not in the dictionary\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "\n",
        "# The dictionary is created from the best \"X\" words (with the highest IG)\n",
        "tokenizer.fit_on_texts(wordVocab[:2155])\n",
        "\n",
        "# The following line of code is with the total dictionary (all words)\n",
        "#tokenizer.fit_on_texts(X_train.tolist())\n",
        "\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "maxlen = 33\n",
        "\n",
        "# We add \"0\" so that all input tensors have the same length, they are parsed to the length of maximum\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njnKJfeEhxsk"
      },
      "source": [
        "# **5. Classification Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gEG0avOh5eL"
      },
      "source": [
        "**The TensorFlow and the Keras library will be used**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmoQUjCRh1Xk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2D-fvJ4rJeJ"
      },
      "source": [
        "For the model, the simplest layers (Dense) will be used, on them, the activation function ReLU will be applied for the first three, and for the last one, Softmax.\n",
        "\n",
        "ReLU activation function: The rectified linear unit activation function (ReLU) is the most widely used activation function for deep learning applications with the most successful and widely used results. The *ReLU function represents a quasi-linear function and therefore retains the properties of linear models, with gradient descent methods. The activation function of ReLU performs a threshold operation for each input element where values less than 0 are set to 0, so the ReLU function is given by:\n",
        "\n",
        "The main advantage of using ReLU in the calculation is that it guarantees a faster calculation, as no exponentials or divisions are calculated, with an overall improved calculation speed.\n",
        "\n",
        "Softmax trigger function: Used to calculate the probability distribution from a vector of real numbers. The Softmax function produces output in a range of values between 0 and 1, with the sum of the probabilities being equal to 1. The Softmax function is calculated using the ratio:\n",
        "\n",
        "The Softmax function is used in multi-class models, where probabilities are returned for each class, with the target class having the highest probability. The Softmax function appears mainly in almost all output layers of deep learning architectures, where they are used.\n",
        "\n",
        "**Application of early-stop to avoid overfitting**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdFlVuiljp_r"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor = 'accuracy', mode = 'max', verbose = 1, patience = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtCLDtjC5f8s"
      },
      "source": [
        "\n",
        "\n",
        "**Cross Validation Hybrid Model**\n",
        "\n",
        "In this code only the training set is used to fit the model. The model is fitted using the model.fit(X[train], y[train]) function within the for train, test in kf.split(X) loop, where X[train] and y[train] are the training sets corresponding to each iteration of the loop. The test set X[test] and y[test] is used only to evaluate the performance of the model after each iteration of the cycle, using the function model.evaluate(X[test], y[test]).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtyj1ltTckEq"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "from keras.layers import Embedding, Flatten, Dense, LSTM, Conv1D, GlobalMaxPooling1D, MaxPooling1D, Bidirectional, GRU\n",
        "from sklearn.model_selection import KFold\n",
        "import statistics\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "#y=np.argmax(y,axis=1)\n",
        "print(type_of_target(y))\n",
        "acc_per_fold=[]\n",
        "loss_per_fold=[]\n",
        "print(type_of_target(y))\n",
        "kf=KFold(n_splits=5, shuffle=True, random_state=999)\n",
        "cvscores=[]\n",
        "for train, test in kf.split(X_train, y_train):\n",
        "  model = Sequential()\n",
        "  embedding_layer = Embedding(vocab_size, 200, input_length=maxlen)\n",
        "\n",
        "  model.add(embedding_layer)\n",
        "\n",
        "  model.add(Conv1D(180, 8, activation='relu'))\n",
        "\n",
        "  model.add(MaxPooling1D(10))\n",
        "\n",
        "  model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.3))\n",
        "\n",
        "\n",
        "  #If we want 3 classes we will use 3 neurons in the last dense layer.\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "\n",
        "  early_stop = EarlyStopping(monitor = 'accuracy', mode = 'max', verbose = 1, patience = 5)\n",
        "  model.fit(X[train], y[train], epochs=100, batch_size=32, verbose=1,validation_data = (X[test], y[test]), callbacks=[early_stop])\n",
        "\n",
        "  scores = model.evaluate(X[test], y[test], verbose=1)\n",
        "  print(f'Score for fold : {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "print(acc_per_fold)\n",
        "print(statistics.mean(acc_per_fold))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "instance = dataset[\"Tweet\"][40]\n",
        "print(instance)\n",
        "\n",
        "\n",
        "instance = tokenizer.texts_to_sequences(instance)\n",
        "\n",
        "flat_list = [] # instance can have more than one sentence, it should be converted into a flat list\n",
        "for sublist in instance: # we go through each sub-list\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)\n",
        "\n",
        "flat_list = [flat_list]\n",
        "instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
        "model.predict(instance) # we use the trained model to predict the instance class"
      ],
      "metadata": {
        "id": "QDXzYf-C1a-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create the following code to insert a sentence and predict the emotion of the sentence a\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Label Positivo is Positive\n",
        "#Label Negativo is Negative\n",
        "#Label Indeterminado is Neutral\n",
        "classes = ['Positivo','Negativo']\n",
        "\n",
        "new_text = input(\"Enter the new phrase to predict its polarity: \")\n",
        "preprocessed_text = [procesado(new_text)]\n",
        "\n",
        "preprocessed_text = [word.lower() for word in preprocessed_text]\n",
        "\n",
        "if len(preprocessed_text) > maxlen:\n",
        "    preprocessed_text = preprocessed_text[:maxlen]\n",
        "\n",
        "# Convert pre-processed text into a sequence of words\n",
        "new_text_sequence = tokenizer.texts_to_sequences(preprocessed_text)\n",
        "# Apply padding to the sequence so that it has the same length.\n",
        "new_text_padded = pad_sequences(new_text_sequence, padding='post', maxlen=maxlen)\n",
        "\n",
        "output_layer = model.layers[-1]\n",
        "model_proba = Model(inputs=model.input, outputs=output_layer.output)\n",
        "\n",
        "# Making the prediction\n",
        "proba = model_proba.predict(new_text_padded)\n",
        "class_probabilities = proba[0]\n",
        "\n",
        "# Print the probability percentages for each class\n",
        "for i, class_probability in enumerate(class_probabilities):\n",
        "    class_label = classes[i]\n",
        "    rounded_probability = round(class_probability * 100, 2)  # Round to 2 decimal places\n",
        "    print(f\"The predicted class for the sentence is '{class_label}': {rounded_probability}%\")\n"
      ],
      "metadata": {
        "id": "ZVepJDSIxadp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ggsUKOj9uh"
      },
      "source": [
        "# **6. Evaluation of the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHng55KckEw0"
      },
      "source": [
        "**A dataframe is to be created containing the values obtained by epoch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi76uc04kFUs"
      },
      "outputs": [],
      "source": [
        "df_modelo = pd.DataFrame(model.history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11Fx33x6kFXK"
      },
      "outputs": [],
      "source": [
        "df_modelo['Epoch'] = range(1, df_modelo.shape[0] + 1)\n",
        "df_modelo.index = df_modelo['Epoch']\n",
        "df_modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f7kQztrkLE7"
      },
      "source": [
        "**The accuracy (accuracy) of the model is then calculated**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QR6rhbvkFZS"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(X_test, y_test, batch_size = 512, verbose = 1)\n",
        "\n",
        "print('\\nAccuracy - Data Test:', round(score[1], 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEXGH7YEkdOy"
      },
      "source": [
        "**Finally, the predictions will be calculated to compute the confusion matrix of the model (confusion_matrix)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s9yl67QkkBS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "prediccion = model.predict(X_test)\n",
        "matriz_confusion = confusion_matrix(np.argmax(y_test, axis = 1), np.argmax(prediccion, axis = 1))\n",
        "df_matriz_confusion = pd.DataFrame(matriz_confusion,\n",
        "                                   index = dataset.columns[-3:],\n",
        "                                    columns = dataset.columns[-3:])\n",
        "plt.figure(figsize = (6, 4))\n",
        "sns.heatmap(df_matriz_confusion, annot = True,fmt='g', annot_kws={\"size\": 14}, cmap = 'BuPu');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OV6IoPLknGa"
      },
      "source": [
        "**A report showing the main metrics of the classification (classification_report) is also created.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3XgvL2hkmg7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(np.argmax(y_test, axis = 1), np.argmax(prediccion, axis = 1)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C6F7sYiTedSz",
        "0ycOQtTJfkDq",
        "Ph8kFdvCgGEr",
        "KWXs7qXHgjVU",
        "njnKJfeEhxsk",
        "o-ggsUKOj9uh"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}